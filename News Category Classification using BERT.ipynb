{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Category Classification Assignnment \n",
    "\n",
    "                                                                                        -- Submitted By Rinki Chatterjee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/news-category-dataset/News_Category_Dataset_v2.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "        \n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /usr/share/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "import transformers #huggingface transformers library\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
    "from keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "from keras.layers.merge import add\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = ToktokTokenizer()\n",
    "nlp = spacy.load('en_core_web_sm', parse = False, tag=False, entity=False)\n",
    "#from contractions import CONTRACTION_MAP\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('/kaggle/input/news-category-dataset/News_Category_Dataset_v2.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# checking TPU\n",
    "try:\n",
    "    \n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "# defining all functions\n",
    "# Remove any emails \n",
    "def remove_emails(text):\n",
    "    text = re.sub(r'\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_hyperlink(text):\n",
    "    text=re.sub(r'(http|https)://[^\\s]*',' ',text)\n",
    "    return text\n",
    "\n",
    "# Removing Digits\n",
    "def remove_digits(text):\n",
    "    #text= re.sub(r\"\\b\\d+\\b\", \"\", text)\n",
    "    text= re.sub(r\"(\\s\\d+)\", \" \", text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "# Removing Special Characters\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-Z\\s]', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# removing accented charactors\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    " # Removing Stopwords\n",
    "def remove_stopwords(text,is_lower_case):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)   \n",
    "    return filtered_text\n",
    "\n",
    "# Lemmetization\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "# Combine all the functions and creating a preprocessing pipeline\n",
    "# # Text preprocessing\n",
    "def text_preprocessing(corpus,isRemoveEmail,isRemoveDigits,isRemoveHyperLink, \n",
    "                     isRemoveSpecialCharac,isRemoveAccentChar,\n",
    "                       text_lower_case,text_lemmatization, stopword_removal):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    \n",
    "    for doc in corpus:\n",
    "        \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        \n",
    "        if isRemoveEmail:\n",
    "            doc = remove_emails(doc)\n",
    "        \n",
    "        if isRemoveHyperLink:\n",
    "            doc=remove_hyperlink(doc)\n",
    "             \n",
    "        if isRemoveAccentChar:\n",
    "            doc = remove_accented_chars(doc)\n",
    "            \n",
    "        if isRemoveDigits:\n",
    "            doc = remove_digits(doc)\n",
    "        \n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        \n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        \n",
    "        if isRemoveSpecialCharac:\n",
    "            doc = remove_special_characters(doc)\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        \n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc,is_lower_case=text_lower_case)\n",
    "                \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_FLAG=True\n",
    "DIGIT_FLAG=True\n",
    "HYPER_LINK_FLAG=True\n",
    "ALL_SPEC_CHAR_FLAG=True\n",
    "ACCENT_CHAR_FLAG=True\n",
    "LOWER_CASE_FLAG=True\n",
    "LEMMETIZE_FLAG=False\n",
    "STOPWORD_FLAG=True\n",
    "\n",
    "clean_headline= text_preprocessing(data['headline'],EMAIL_FLAG,DIGIT_FLAG,HYPER_LINK_FLAG,\n",
    "                   ALL_SPEC_CHAR_FLAG,ACCENT_CHAR_FLAG,\n",
    "                  LOWER_CASE_FLAG,LEMMETIZE_FLAG,STOPWORD_FLAG)\n",
    "clean_short_Desc = text_preprocessing(data['short_description'],EMAIL_FLAG,DIGIT_FLAG,HYPER_LINK_FLAG,\n",
    "                   ALL_SPEC_CHAR_FLAG,ACCENT_CHAR_FLAG,\n",
    "                  LOWER_CASE_FLAG,LEMMETIZE_FLAG,STOPWORD_FLAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_headline']=clean_headline\n",
    "data['clean_short_Desc'] = clean_short_Desc\n",
    "\n",
    "# Merging both the columns\n",
    "data['MergedColumn'] = data[data.columns[6:8]].apply(\n",
    "    lambda x: ' '.join(x.astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df = data.copy()\n",
    "del data\n",
    "df.drop(columns=['headline', 'authors', 'link', 'short_description', 'date',\n",
    "                   'clean_headline', 'clean_short_Desc'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 41 unique categories\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset contains { df.category.nunique() } unique categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding the categories. After this each category would be mapped to an integer.\n",
    "encoder = LabelEncoder()\n",
    "df['categoryEncoded'] = encoder.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face tokenizer for converting the data in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using hugging face tokenizer\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8808d8f91442fdb7744d6c683b9925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bert large uncased pretrained tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test ,y_train,y_test = train_test_split(df['MergedColumn'], df['categoryEncoded'], random_state = 2020, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the news descriptions and converting the categories into one hot vectors using tf.keras.utils.to_categorical\n",
    "Xtrain_encoded = regular_encode(X_train.astype('str'), tokenizer, maxlen=80)\n",
    "ytrain_encoded = tf.keras.utils.to_categorical(y_train, num_classes=41,dtype = 'int32')\n",
    "Xtest_encoded = regular_encode(X_test.astype('str'), tokenizer, maxlen=80)\n",
    "ytest_encoded = tf.keras.utils.to_categorical(y_test, num_classes=41,dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, loss='categorical_crossentropy', max_len=512):\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    #adding dropout layer\n",
    "    x = tf.keras.layers.Dropout(0.3)(cls_token)\n",
    "    #using a dense layer of 41 neurons as the number of unique categories is 41. \n",
    "    out = tf.keras.layers.Dense(41, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n",
    "    #using categorical crossentropy as the loss as it is a multi-class classification problem\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=3e-5), loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90760fa391db43edb241993a3f189ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdb9ce0f8374e6a9632ddf713d79225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1472569832.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 80)]              0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model (TFBertModel)  ((None, 80, 1024), (None, 335141888 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 41)                42025     \n",
      "=================================================================\n",
      "Total params: 335,183,913\n",
      "Trainable params: 335,183,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model on tpu\n",
    "with strategy.scope():\n",
    "    transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n",
    "    model = build_model(transformer_layer, max_len=80)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the training and testing dataset.\n",
    "BATCH_SIZE = 32*strategy.num_replicas_in_sync\n",
    "AUTO = tf.data.experimental.AUTOTUNE \n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((Xtrain_encoded, ytrain_encoded))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(Xtest_encoded)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "549/549 [==============================] - 179s 327ms/step - loss: 2.1144 - accuracy: 0.4556\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 179s 327ms/step - loss: 1.2762 - accuracy: 0.6445\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 180s 327ms/step - loss: 1.0530 - accuracy: 0.6961\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 180s 327ms/step - loss: 0.8875 - accuracy: 0.7375\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 180s 327ms/step - loss: 0.7224 - accuracy: 0.7809\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 179s 327ms/step - loss: 0.5863 - accuracy: 0.8188\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 180s 327ms/step - loss: 0.4748 - accuracy: 0.8518\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 180s 327ms/step - loss: 0.3745 - accuracy: 0.8820\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 180s 327ms/step - loss: 0.2890 - accuracy: 0.9091\n",
      "Epoch 10/10\n",
      "549/549 [==============================] - 180s 328ms/step - loss: 0.2251 - accuracy: 0.9280\n"
     ]
    }
   ],
   "source": [
    "#training for 10 epochs\n",
    "n_steps = Xtrain_encoded.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 32s 136ms/step\n"
     ]
    }
   ],
   "source": [
    "#making predictions\n",
    "preds = model.predict(test_dataset,verbose = 1)\n",
    "#converting the one hot vector output to a linear numpy array.\n",
    "pred_classes = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the classes from the label encoder\n",
    "encoded_classes = encoder.classes_\n",
    "#mapping the encoded output to actual categories\n",
    "predicted_category = [encoded_classes[x] for x in pred_classes]\n",
    "true_category = [encoded_classes[x] for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>true_category</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75589</th>\n",
       "      <td>huffpost rise morning newsbrief november welco...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21665</th>\n",
       "      <td>donald trump lawyer claims president never tol...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51481</th>\n",
       "      <td>feminist comic series fans stranger things sat...</td>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20578</th>\n",
       "      <td>obamacare repeal moves ahead key senate vote s...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58445</th>\n",
       "      <td>mount holyoke commencement speaker thanks acti...</td>\n",
       "      <td>COLLEGE</td>\n",
       "      <td>BLACK VOICES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description   true_category  \\\n",
       "75589  huffpost rise morning newsbrief november welco...        POLITICS   \n",
       "21665  donald trump lawyer claims president never tol...        POLITICS   \n",
       "51481  feminist comic series fans stranger things sat...  ARTS & CULTURE   \n",
       "20578  obamacare repeal moves ahead key senate vote s...        POLITICS   \n",
       "58445  mount holyoke commencement speaker thanks acti...         COLLEGE   \n",
       "\n",
       "      predicted_category  \n",
       "75589           POLITICS  \n",
       "21665           POLITICS  \n",
       "51481     ARTS & CULTURE  \n",
       "20578           POLITICS  \n",
       "58445       BLACK VOICES  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame({'description':X_test,'true_category':true_category, 'predicted_category':predicted_category})\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.6571627721720659\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy is {sklearn.metrics.accuracy_score(result_df['true_category'], result_df['predicted_category'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('Predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>true_category</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58445</th>\n",
       "      <td>mount holyoke commencement speaker thanks acti...</td>\n",
       "      <td>COLLEGE</td>\n",
       "      <td>BLACK VOICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120202</th>\n",
       "      <td>tenure education friend foe common public misp...</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>COLLEGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182595</th>\n",
       "      <td>criminal ethical presume society living rule l...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25864</th>\n",
       "      <td>manchester blood banks receive overwhelming nu...</td>\n",
       "      <td>THE WORLDPOST</td>\n",
       "      <td>WORLD NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112055</th>\n",
       "      <td>standing tall native american day unprecedente...</td>\n",
       "      <td>MEDIA</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9725</th>\n",
       "      <td>traveling ethiopia taught appreciate heritage ...</td>\n",
       "      <td>BLACK VOICES</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148955</th>\n",
       "      <td>essentials vegan kitchen time come dispel many...</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118713</th>\n",
       "      <td>conservatives like taxes lessons running offic...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57660</th>\n",
       "      <td>spoof gum commercial chews away islamophobia b...</td>\n",
       "      <td>RELIGION</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65368</th>\n",
       "      <td>white people heroin overdose deaths tripled am...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>HEALTHY LIVING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20658 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description  true_category  \\\n",
       "58445   mount holyoke commencement speaker thanks acti...        COLLEGE   \n",
       "120202  tenure education friend foe common public misp...      EDUCATION   \n",
       "182595  criminal ethical presume society living rule l...       BUSINESS   \n",
       "25864   manchester blood banks receive overwhelming nu...  THE WORLDPOST   \n",
       "112055  standing tall native american day unprecedente...          MEDIA   \n",
       "...                                                   ...            ...   \n",
       "9725    traveling ethiopia taught appreciate heritage ...   BLACK VOICES   \n",
       "148955  essentials vegan kitchen time come dispel many...       WELLNESS   \n",
       "118713  conservatives like taxes lessons running offic...       POLITICS   \n",
       "57660   spoof gum commercial chews away islamophobia b...       RELIGION   \n",
       "65368   white people heroin overdose deaths tripled am...       POLITICS   \n",
       "\n",
       "       predicted_category  \n",
       "58445        BLACK VOICES  \n",
       "120202            COLLEGE  \n",
       "182595              CRIME  \n",
       "25864          WORLD NEWS  \n",
       "112055           POLITICS  \n",
       "...                   ...  \n",
       "9725               TRAVEL  \n",
       "148955       FOOD & DRINK  \n",
       "118713              GREEN  \n",
       "57660              COMEDY  \n",
       "65368      HEALTHY LIVING  \n",
       "\n",
       "[20658 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[result_df['true_category']!=result_df['predicted_category']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
